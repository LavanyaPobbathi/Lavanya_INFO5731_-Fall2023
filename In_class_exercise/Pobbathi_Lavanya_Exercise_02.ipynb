{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaPobbathi/Lavanya_INFO5731_Fall2023/blob/main/In_class_exercise/Pobbathi_Lavanya_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW5_oFVd9-pY"
      },
      "source": [
        "## The second In-class-exercise (09/13/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAzh1U0sE5I5"
      },
      "source": [
        "Kindly use the provided .ipynb document to write your code or respond to the questions. Avoid generating a new file.\n",
        "Execute all the cells before your final submission."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpgvZQdRE-HV"
      },
      "source": [
        "This in-class exercise is due tomorrow September 14, 2023 at 11:59 PM. No late submissions will be considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QBZI-je9-pZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWoKpYQT9-pa"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-LmNR3kw9-pa",
        "outputId": "4ff922d3-b1bc-4532-c170-a6c29ba7efd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nResearch Question: How does age impact Body Mass Index (BMI) and its classification in a specific population, and what insights can this provide for healthcare interventions?\\n\\nData Needed:\\nTo answer this research question, need the following data:\\n\\nAge Data: Collect data on the ages of individuals within the specific population.\\nHeight Data: Gather information about the heights of these individuals.\\nWeight Data: Obtain data regarding the weights of the individuals.\\nBMI Data: Calculate BMI for each individual using the formula: BMI = (Weight in kg) / (Height in meters)^2.\\nBMI Class Data: Determine the BMI classification for each individual (e.g., underweight, normal weight, overweight, obese) based on BMI values.\\n\\nNumber of Data Points Needed:\\n\\nThe number of data points required will depend on the size and diversity of your population. To draw meaningful conclusions, you should aim for a sufficiently large and diverse sample, which might involve data for hundreds or thousands of individuals.\\n\\nSteps for Collecting and Saving the Data:\\n\\nData Collection: Gather the data from individuals within your chosen population. This can be done through surveys, electronic health records, or physical measurements.\\nData Cleaning: Clean the collected data to handle any missing values or outliers. Ensure all data is in a consistent format.\\nBMI Calculation: Calculate BMI for each individual using their weight and height data. Convert height from centimeters to meters for the calculation.\\nBMI Classification: Determine the BMI class for each individual based on their calculated BMI values and predefined BMI ranges for classifications.\\nData Integration: Combine all the collected and computed data into a single dataset. Ensure that there is a common identifier (e.g., unique ID) to link the data.\\nData Storage: Save the integrated dataset in a secure and organized manner. You can use a spreadsheet format (e.g., CSV) or a relational database.\\nData Backup: Implement regular backup procedures to prevent data loss.\\nData Analysis: Utilize statistical analysis techniques to explore how age impacts BMI and BMI classification within the population.\\n\\nVisualization and Reporting: Create visualizations and reports to convey your findings effectively.\\n\\nEthical Considerations: Ensure compliance with data privacy and ethical guidelines, especially when dealing with health-related data. Protect individuals' privacy and anonymize data if necessary.\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Research Question: How does age impact Body Mass Index (BMI) and its classification in a specific population, and what insights can this provide for healthcare interventions?\n",
        "\n",
        "Data Needed:\n",
        "To answer this research question, need the following data:\n",
        "\n",
        "Age Data: Collect data on the ages of individuals within the specific population.\n",
        "Height Data: Gather information about the heights of these individuals.\n",
        "Weight Data: Obtain data regarding the weights of the individuals.\n",
        "BMI Data: Calculate BMI for each individual using the formula: BMI = (Weight in kg) / (Height in meters)^2.\n",
        "BMI Class Data: Determine the BMI classification for each individual (e.g., underweight, normal weight, overweight, obese) based on BMI values.\n",
        "\n",
        "Number of Data Points Needed:\n",
        "\n",
        "The number of data points required will depend on the size and diversity of your population. To draw meaningful conclusions, you should aim for a sufficiently large and diverse sample, which might involve data for hundreds or thousands of individuals.\n",
        "\n",
        "Steps for Collecting and Saving the Data:\n",
        "\n",
        "Data Collection: Gather the data from individuals within your chosen population. This can be done through surveys, electronic health records, or physical measurements.\n",
        "Data Cleaning: Clean the collected data to handle any missing values or outliers. Ensure all data is in a consistent format.\n",
        "BMI Calculation: Calculate BMI for each individual using their weight and height data. Convert height from centimeters to meters for the calculation.\n",
        "BMI Classification: Determine the BMI class for each individual based on their calculated BMI values and predefined BMI ranges for classifications.\n",
        "Data Integration: Combine all the collected and computed data into a single dataset. Ensure that there is a common identifier (e.g., unique ID) to link the data.\n",
        "Data Storage: Save the integrated dataset in a secure and organized manner. You can use a spreadsheet format (e.g., CSV) or a relational database.\n",
        "Data Backup: Implement regular backup procedures to prevent data loss.\n",
        "Data Analysis: Utilize statistical analysis techniques to explore how age impacts BMI and BMI classification within the population.\n",
        "\n",
        "Visualization and Reporting: Create visualizations and reports to convey your findings effectively.\n",
        "\n",
        "Ethical Considerations: Ensure compliance with data privacy and ethical guidelines, especially when dealing with health-related data. Protect individuals' privacy and anonymize data if necessary.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxTLRNm9-pa"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpWOgjHi9-pa",
        "outputId": "374ee956-b3cf-43c4-b084-f899aee2e3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data collection and saving complete.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize empty lists for data\n",
        "age_data = []\n",
        "height_data = []\n",
        "weight_data = []\n",
        "\n",
        "# Generate random data for 1000 individuals\n",
        "for _ in range(1000):\n",
        "    age = random.randint(18, 80)  # Random age between 18 and 80\n",
        "    height = random.uniform(140, 200)  # Random height in centimeters (140 cm to 200 cm)\n",
        "    weight = random.uniform(40, 120)  # Random weight in kilograms (40 kg to 120 kg)\n",
        "\n",
        "    # Calculate BMI\n",
        "    bmi = weight / ((height / 100) ** 2)\n",
        "\n",
        "    # Determine BMI class based on BMI value\n",
        "    if bmi < 18.5:\n",
        "        bmi_class = \"Underweight\"\n",
        "    elif 18.5 <= bmi < 24.9:\n",
        "        bmi_class = \"Normal Weight\"\n",
        "    elif 25 <= bmi < 29.9:\n",
        "        bmi_class = \"Overweight\"\n",
        "    else:\n",
        "        bmi_class = \"Obese\"\n",
        "\n",
        "    # Append data to respective lists\n",
        "    age_data.append(age)\n",
        "    height_data.append(height)\n",
        "    weight_data.append(weight)\n",
        "\n",
        "# Create a DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'Age': age_data,\n",
        "    'Height (cm)': height_data,\n",
        "    'Weight (kg)': weight_data,\n",
        "    'BMI': [round(weight / ((height / 100) ** 2), 2) for weight, height in zip(weight_data, height_data)],\n",
        "    'BMI Class': [bmi_class for _ in range(1000)]\n",
        "})\n",
        "\n",
        "# Save the data to a CSV file\n",
        "data.to_csv('bmi.csv', index=False)\n",
        "\n",
        "print(\"Data collection and saving complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6wgvog9-pa"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2013-2023).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5rjlclf9-pb",
        "outputId": "300bd068-2e21-446c-a3cb-06d4c943d0f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected 600 articles and saved to 'articles.json'.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Function to fetch articles from Google Scholar\n",
        "def fetch_google_scholar_articles(query, start_year, end_year, num_articles):\n",
        "    # Base URL for Google Scholar\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    articles = []  # List to store the collected articles\n",
        "\n",
        "    # Loop to paginate through search results (10 results per page)\n",
        "    for start in range(0, num_articles, 10):\n",
        "        # Parameters for the search query, including keywords and date range\n",
        "        params = {\n",
        "            \"q\": query,             # Search query\n",
        "            \"as_ylo\": start_year,   # Start year of publication range\n",
        "            \"as_yhi\": end_year,     # End year of publication range\n",
        "            \"start\": start          # Pagination offset\n",
        "        }\n",
        "\n",
        "        # User-Agent header to mimic a web browser\n",
        "        headers = {\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.1234.0 Safari/537.36\"\n",
        "        }\n",
        "\n",
        "        # Send a GET request to the Google Scholar search URL with parameters and headers\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "\n",
        "        # Check if the response is successful (HTTP status code 200)\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the response using BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Find all the search result div elements\n",
        "            results = soup.find_all('div', {'class': 'gs_ri'})\n",
        "\n",
        "            # Iterate through each search result\n",
        "            for result in results:\n",
        "                article = {}  # Dictionary to store article information\n",
        "\n",
        "                # Extract the title of the article (inside an h3 element with class 'gs_rt')\n",
        "                title = result.find('h3', {'class': 'gs_rt'})\n",
        "                if title:\n",
        "                    article['title'] = title.text  # Store the title in the dictionary\n",
        "\n",
        "                # Extract the venue/journal/conference information (inside a div element with class 'gs_a')\n",
        "                venue = result.find('div', {'class': 'gs_a'})\n",
        "                if venue:\n",
        "                    article['venue'] = venue.text  # Store the venue information\n",
        "\n",
        "                # Extract the publication year (from the 'gs_a' div)\n",
        "                year = result.find('div', {'class': 'gs_a'})\n",
        "                if year:\n",
        "                    # Split the text and get the last part (usually the year), then strip whitespace\n",
        "                    year = year.text.split('-')[-1].strip()\n",
        "                    article['year'] = year  # Store the year in the dictionary\n",
        "\n",
        "                # Extract the authors (from the 'gs_a' div)\n",
        "                authors = result.find('div', {'class': 'gs_a'})\n",
        "                if authors:\n",
        "                    # Split the text and get the first part (usually the authors), then strip whitespace\n",
        "                    authors = authors.text.split('-')[0].strip()\n",
        "                    article['authors'] = authors  # Store the authors in the dictionary\n",
        "\n",
        "                # Extract the abstract (inside a div element with class 'gs_rs')\n",
        "                abstract = result.find('div', {'class': 'gs_rs'})\n",
        "                if abstract:\n",
        "                    article['abstract'] = abstract.text  # Store the abstract in the dictionary\n",
        "\n",
        "                # Append the article dictionary to the list of articles\n",
        "                articles.append(article)\n",
        "\n",
        "                # Check if the desired number of articles has been collected\n",
        "                if len(articles) >= num_articles:\n",
        "                    return articles\n",
        "\n",
        "    return articles\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    keyword = \"information retrieval\"  # Keyword for the search\n",
        "    start_year = 2013                # Start year of publication range\n",
        "    end_year = 2023                  # End year of publication range\n",
        "    num_articles = 1000              # Desired number of articles to collect\n",
        "\n",
        "    # Call the fetch_google_scholar_articles function to collect articles\n",
        "    articles = fetch_google_scholar_articles(keyword, start_year, end_year, num_articles)\n",
        "\n",
        "    # Save the collected articles to a JSON file\n",
        "    with open(\"articles.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
        "        json.dump(articles, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    # Print the number of collected articles and a confirmation message\n",
        "    print(f\"Collected {len(articles)} articles and saved to 'articles.json'.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3CNj_V9-pb"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data.\n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viY1VDrsC2py",
        "outputId": "3a489913-e0c0-4be9-ba32-cc931fb2246b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install --upgrade praw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BcaVllQQjm9",
        "outputId": "c8e2e3af-1b1d-4b52-957f-2f81cfac6e55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.3.0 update-checker-0.18.0\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.3.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.6.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You code here (Please add comments in the code):\n",
        "#getting the posts from reddit platform\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the Reddit API client\n",
        "reddit_posts = praw.Reddit(client_id = \"CLIENT_ID\", #peronal use script\n",
        "                    client_secret = \"CLIENT_SECRET\", #secret token\n",
        "                    usernme = \"USERNAME\", #profile username\n",
        "                    password = \"PASSWORD\", #profile password\n",
        "                    user_agent = \"USERAGENT\", #profile agent\n",
        "                    check_for_async=False)\n",
        "\n",
        "# List of subreddits to scrape\n",
        "subreddit_list_posts =  ['india','worldnews','announcements','funny','AskReddit',\n",
        "                  'gaming','pics','science','movies','todayilearned'\n",
        "                 ]\n",
        "\n",
        "# Initialize lists to store post data\n",
        "author = []\n",
        "id = []\n",
        "link_flair_text = []\n",
        "num_comments = []\n",
        "score = []\n",
        "title = []\n",
        "upvote_ratio = []\n",
        "\n",
        "# Loop through each subreddit\n",
        "for subred in subreddit_list_posts:\n",
        "    # Access the subreddit\n",
        "    subreddit = reddit_posts.subreddit(subred)\n",
        "\n",
        "    # Get the top 1000 hot posts from the subreddit\n",
        "    hot_post = subreddit.hot(limit=1000)\n",
        "\n",
        "    # Extract desired data from each post and store in respective lists\n",
        "    for sub in hot_post:\n",
        "        author.append(sub.author)\n",
        "        id.append(sub.id)\n",
        "        link_flair_text.append(sub.link_flair_text)\n",
        "        num_comments.append(sub.num_comments)\n",
        "        score.append(sub.score)\n",
        "        title.append(sub.title)\n",
        "        upvote_ratio.append(sub.upvote_ratio)\n",
        "\n",
        "    # Print progress for each subreddit\n",
        "    print(subred, 'completed; ', end='')\n",
        "    print('total', len(author), 'posts have been scraped')\n",
        "\n",
        "# Create a dataframe using the extracted data\n",
        "df = pd.DataFrame({'ID':id,\n",
        "                   'Author':author,\n",
        "                   'Title':title,\n",
        "                   'Count_of_Comments':num_comments,\n",
        "                   'Upvote_Count':score,\n",
        "                   'Upvote_Ratio':upvote_ratio,\n",
        "                   'Flair':link_flair_text\n",
        "                  })\n",
        "df.to_csv('reddit_dataset.csv', index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAae59w5UGJf",
        "outputId": "528bf90b-8c41-46dc-d83c-3dbd99550b6f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "india completed; total 555 posts have been scraped\n",
            "worldnews completed; total 1226 posts have been scraped\n",
            "announcements completed; total 1392 posts have been scraped\n",
            "funny completed; total 1758 posts have been scraped\n",
            "AskReddit completed; total 2542 posts have been scraped\n",
            "gaming completed; total 2755 posts have been scraped\n",
            "pics completed; total 3683 posts have been scraped\n",
            "science completed; total 4459 posts have been scraped\n",
            "movies completed; total 4898 posts have been scraped\n",
            "todayilearned completed; total 5437 posts have been scraped\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}